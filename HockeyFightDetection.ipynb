{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install pytorch-ignite\n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "import torch\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import torchvision\n",
        "import cv2\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from ignite.metrics import Accuracy, Recall, Precision\n",
        "from ignite.engine import create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.engine.events import Events\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "dBpEOq_hte18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "!mkdir models\n",
        "!mkdir data"
      ],
      "metadata": {
        "id": "AmaoHCsrtdEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecursiveCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(RecursiveCNN, self).__init__()\n",
        "        self.device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        \n",
        "        self.feature_extractor = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        self.feature_extractor.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(p=0.5))\n",
        "\n",
        "        self.rnn = nn.LSTM(input_size=4096, hidden_size=1024, num_layers=1)\n",
        "\n",
        "        self.output_fc = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 64),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(64, num_classes),\n",
        "            nn.Softmax(dim=1) )\n",
        "        \n",
        "    def forward(self, x): #input should be a batch of videos of below shape\n",
        "        b_z, length, colors, height, width = x.shape\n",
        "        hidden_state = None\n",
        "        for frame in range(length):#for every image in the video (all samples in the batch at once)\n",
        "            with torch.no_grad():\n",
        "                cnn_out = self.feature_extractor((x[:,frame]))\n",
        "            lstm_out, hidden_state = self.rnn(cnn_out.unsqueeze(0), hidden_state) #rnn takes input of shape (sequence_length, batch_size, input_size)\n",
        "            del cnn_out\n",
        "        del hidden_state\n",
        "        return self.output_fc(lstm_out.squeeze(0))\n",
        "\n",
        "\n",
        "class HockeyDataset(Dataset):\n",
        "    def __init__(self, device, max_frames, image_size, start_idx=0, end_idx=500):\n",
        "        super(HockeyDataset, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.max_frames = max_frames\n",
        "        self.fight_folder = '/content/drive/MyDrive/hockey_data/dataset/fight/'\n",
        "        self.nofight_folder = '/content/drive/MyDrive/hockey_data/dataset/no_fight/'\n",
        "        self.device = device\n",
        "        fight_filenames = [f for f in listdir(self.fight_folder) if isfile(join(self.fight_folder, f))]\n",
        "        no_fight_filenames = [f for f in listdir(self.nofight_folder) if isfile(join(self.nofight_folder, f))]\n",
        "        self.filenames = fight_filenames[start_idx:end_idx]\n",
        "        self.filenames.extend(no_fight_filenames[start_idx:end_idx])\n",
        "        self.num_classes = 2\n",
        "        self.end_idx = end_idx\n",
        "        self.start_idx = start_idx\n",
        "\n",
        "    def preprocessSample(self, video):\n",
        "        augmentations = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Normalize([0.43216, 0.394666, 0.37645], [0.22803, 0.22145, 0.216989])\n",
        "            ])\n",
        "        return augmentations(video)\n",
        "    def video_to_frames(self, video_path, size, max_frames):\n",
        "        video = cv2.VideoCapture(video_path)\n",
        "        if not video.isOpened():\n",
        "            video.release()\n",
        "            print('Could not open video. Check given path: ' + str(video_path))\n",
        "            return None\n",
        "        frames = []\n",
        "        frameCount = 1 #not 0 since frameStart begins at 1 as well\n",
        "        while True:\n",
        "            framesLeft, frame = video.read()\n",
        "            if framesLeft:\n",
        "                if max_frames < frameCount: break #do not store frames after our end point\n",
        "                toTensor = torchvision.transforms.ToTensor()\n",
        "                frame = toTensor(frame)\n",
        "                resize = torchvision.transforms.Resize(size)\n",
        "                frame = resize(frame)\n",
        "                frames.append(frame)\n",
        "                frameCount += 1\n",
        "            else:\n",
        "                break\n",
        "        video.release()\n",
        "        tensor_frames = torch.empty((len(frames), 3, size[0], size[1]))\n",
        "        for i in range(len(tensor_frames)):\n",
        "            tensor_frames[i] = frames[i]\n",
        "        del frames\n",
        "        return tensor_frames\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.filenames[idx]\n",
        "        if idx < self.end_idx-self.start_idx:\n",
        "            filename = self.fight_folder + filename\n",
        "            label = np.array([1, 0])\n",
        "        else:\n",
        "            filename = self.nofight_folder + filename\n",
        "            label = np.array([0, 1])\n",
        "        sample = self.video_to_frames(filename, self.image_size, self.max_frames)\n",
        "        sample = self.preprocessSample(sample)\n",
        "        return sample, label\n",
        "\n",
        "\n",
        "def start_training(train_loader, test_loader, epochs, model, optimizer, loss, metrics, gradient_accumulation_steps): \n",
        "        trainer = create_supervised_trainer(model, optimizer, loss, gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "        evaluator = create_supervised_evaluator(model, metrics=metrics)\n",
        "        \n",
        "        @trainer.on(Events.ITERATION_COMPLETED)\n",
        "        def log_batch_complete():\n",
        "            print(f\"epoch {trainer.state.epoch} iteration {trainer.state.iteration} completed.\")\n",
        "\n",
        "        @trainer.on(Events.EPOCH_COMPLETED)\n",
        "        def log_epoch_time():\n",
        "            print(f\"epoch {trainer.state.epoch} completed, time elapsed : {trainer.state.times['EPOCH_COMPLETED']}\")\n",
        "            evaluate()\n",
        "\n",
        "        def evaluate():\n",
        "            print(\"evaluating model\")\n",
        "            evaluator.run(test_loader)\n",
        "            metrics = dict(evaluator.state.metrics)\n",
        "            print(str(metrics))\n",
        "            torch.save(model.state_dict(), 'models/model_e' + str(trainer.state.epoch) + \"_accuracy_\" + str(metrics['accuracy']) + \".pt\")\n",
        "        trainer.run(train_loader, max_epochs=epochs)\n",
        "        evaluator.run(test_loader)\n",
        "        return \n",
        "        \n",
        "def custom_collate_fn(data): #data is a list of [batch_size] tuples of the form (video, label), we need to return the padded videos and the corresponding labels as tensors\n",
        "    device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "    videos = []\n",
        "    labels = torch.zeros((len(data), len(data[0][1])))\n",
        "    for i in range(len(data)):\n",
        "        sample = data[i]\n",
        "        videos.append(sample[0])\n",
        "        labels[i][0] = sample[1][0]\n",
        "        labels[i][1] = sample[1][1]\n",
        "    videos = rnn_utils.pad_sequence(videos, batch_first=True, padding_value=0.0)\n",
        "    del data\n",
        "    return videos.to(device), labels.to(device)\n",
        "\n",
        "def getMetricInFilename(f):\n",
        "    metric_in_filename = re.findall('\\d*\\.?\\d+',f)[1]\n",
        "    return metric_in_filename\n",
        "\n",
        "def roundToLabel(output):\n",
        "    y_pred, y = output\n",
        "    for out in y_pred:\n",
        "        if out[1] < out[0]: \n",
        "            out[0] = 1.0\n",
        "            out[1]= 0.0\n",
        "        else:\n",
        "            out[0] = 0.0\n",
        "            out[1]= 1.0\n",
        "    return y_pred, y\n",
        "\n",
        "def launch(data_splits, batch_sizes, max_frames, device, profile, epochs, image_size, gradient_accumulation_steps, force_generate_dataset, continuous_training, samples_per_label):\n",
        "    print(\"starting on \" + str(device))\n",
        "    #load dataset from file system or from scratch\n",
        "    try:\n",
        "        if force_generate_dataset: raise Exception\n",
        "        train_ds, test_ds, val_ds = torch.load(\"data/datasets.sav\")\n",
        "        print(\"loaded datasets from file system\")\n",
        "    except Exception as e:\n",
        "        print(\"generating dataset from scratch\")\n",
        "        #there are 1000 videos (500 per label) so stratified split is easy\n",
        "        split_per_label = [samples_per_label*x for x in data_splits]\n",
        "        train_ds = HockeyDataset(device, max_frames, image_size, 0, int(split_per_label[0]))\n",
        "        test_ds = HockeyDataset(device, max_frames, image_size, int(split_per_label[0]), int(split_per_label[0]+split_per_label[1]))\n",
        "        val_ds = HockeyDataset(device, max_frames, image_size, int(split_per_label[0]+split_per_label[1]), int(split_per_label[0]+split_per_label[1]+split_per_label[2]))\n",
        "        torch.save((train_ds, test_ds, val_ds), \"data/datasets.sav\")\n",
        "\n",
        "    print(\"setting up dataloaders, metrics and model\")\n",
        "    # split into 3 dataloaders (for train, test, and valid)\n",
        "    train_dl = DataLoader(train_ds, batch_size = batch_sizes[0], shuffle=True, collate_fn= custom_collate_fn)\n",
        "    test_dl = DataLoader(test_ds, batch_size = batch_sizes[1], shuffle=True, collate_fn= custom_collate_fn)\n",
        "    val_dl = DataLoader(val_ds, batch_size = batch_sizes[2], shuffle=True, collate_fn= custom_collate_fn)\n",
        "    #load model\n",
        "    if continuous_training:\n",
        "        files = [f for f in listdir('models/') if isfile(join('models/', f))]\n",
        "        noModels = False\n",
        "        if len(files) == 0:\n",
        "            print(\"No model file to continue training from.\")\n",
        "            exit()\n",
        "        best_model = max([getMetricInFilename(f) for f in files])\n",
        "        for f in files:\n",
        "            if best_model not in f: continue\n",
        "            model.load_state_dict(torch.load(f))    \n",
        "            print(\"continuing from: \" + f)\n",
        "    else:\n",
        "        model = RecursiveCNN(train_ds.num_classes)\n",
        "    model.to(device)\n",
        "    loss = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    metrics = {'accuracy': Accuracy(device=device, output_transform=roundToLabel), 'recall': Recall(device=device), 'precision':Precision(device=device)}\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Starting training with \" + str(len(train_ds)) + \" out of 1000 samples.\")\n",
        "    if profile:\n",
        "        with torch.autograd.profiler.profile() as prof:\n",
        "            try:\n",
        "                measured_metrics = start_training(train_dl, test_dl, epochs, model, optimizer, loss, metrics, gradient_accumulation_steps)\n",
        "            except Exception as e:\n",
        "                print(\"ended with error \" + str(e))\n",
        "            print(prof.key_averages())\n",
        "    else:\n",
        "        measured_metrics = start_training(train_dl, test_dl, epochs, model, optimizer, loss, metrics, gradient_accumulation_steps)\n",
        "    \n",
        "    print(\"done!\")"
      ],
      "metadata": {
        "id": "aIorXTi2tj8N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "34c32532-7277-4b3b-8c4b-9d57d082fdcb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cfdec09920a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRecursiveCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRecursiveCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_splits = [0.7, 0.2, 0.1]\n",
        "batch_sizes = [16, 16, 8]\n",
        "gradient_accumulation_steps = 1\n",
        "image_size = (224, 224)\n",
        "samples_per_label = 500\n",
        "max_frames = 20 #no more than _ frames\n",
        "device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "epochs = 50\n",
        "profile = True\n",
        "force_generate_dataset = True #when True re-generates the dataset object instead of loading from file system\n",
        "continuous_training = False\n",
        "launch(data_splits, batch_sizes, max_frames, device, profile, epochs, image_size, gradient_accumulation_steps, force_generate_dataset, continuous_training, samples_per_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogm9Mv6Cuyqx",
        "outputId": "1546216a-14f0-4697-f426-7d0d811c0d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting on cuda\n",
            "generating dataset from scratch\n",
            "setting up dataloaders, metrics and model\n",
            "Starting training with 700 out of 1000 samples.\n",
            "epoch 1 iteration 1 completed.\n",
            "epoch 1 iteration 2 completed.\n",
            "epoch 1 iteration 3 completed.\n",
            "epoch 1 iteration 4 completed.\n",
            "epoch 1 iteration 5 completed.\n",
            "epoch 1 iteration 6 completed.\n",
            "epoch 1 iteration 7 completed.\n",
            "epoch 1 iteration 8 completed.\n",
            "epoch 1 iteration 9 completed.\n",
            "epoch 1 iteration 10 completed.\n",
            "epoch 1 iteration 11 completed.\n",
            "epoch 1 iteration 12 completed.\n",
            "epoch 1 iteration 13 completed.\n",
            "epoch 1 iteration 14 completed.\n",
            "epoch 1 iteration 15 completed.\n",
            "epoch 1 iteration 16 completed.\n",
            "epoch 1 iteration 17 completed.\n",
            "epoch 1 iteration 18 completed.\n",
            "epoch 1 iteration 19 completed.\n",
            "epoch 1 iteration 20 completed.\n",
            "epoch 1 iteration 21 completed.\n",
            "epoch 1 iteration 22 completed.\n",
            "epoch 1 iteration 23 completed.\n",
            "epoch 1 iteration 24 completed.\n",
            "epoch 1 iteration 25 completed.\n",
            "epoch 1 iteration 26 completed.\n",
            "epoch 1 iteration 27 completed.\n",
            "epoch 1 iteration 28 completed.\n",
            "epoch 1 iteration 29 completed.\n",
            "epoch 1 iteration 30 completed.\n",
            "epoch 1 iteration 31 completed.\n",
            "epoch 1 iteration 32 completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import psutil\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "def memReport():\n",
        "    for obj in gc.get_objects():\n",
        "        try:\n",
        "            if torch.is_tensor(obj):\n",
        "                print(type(obj), obj.size())\n",
        "                del obj\n",
        "        except:\n",
        "            continue\n",
        "    gc.collect()\n",
        "        \n",
        "def cpuStats():\n",
        "    print(sys.version)\n",
        "    print(psutil.cpu_percent())\n",
        "    print(psutil.virtual_memory())  # physical memory usage\n",
        "    pid = os.getpid()\n",
        "    py = psutil.Process(pid)\n",
        "    memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
        "    print('memory GB:', memoryUse)\n",
        "\n",
        "memReport()\n",
        "cpuStats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glxCYzAM5CFJ",
        "outputId": "1c6a3597-8cd6-454a-9927-f584ec0ec592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8.10 (default, Nov 14 2022, 12:59:47) \n",
            "[GCC 9.4.0]\n",
            "26.7\n",
            "svmem(total=13616324608, available=11847000064, percent=13.0, used=1483288576, free=10924474368, active=407371776, inactive=2031546368, buffers=153669632, cached=1054892032, shared=1343488, slab=117903360)\n",
            "memory GB: 0.297119140625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}